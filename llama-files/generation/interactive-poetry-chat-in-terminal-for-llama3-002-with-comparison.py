import torch
import sys 
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TextStreamer
)
from peft import PeftModel

# --- 1. Custom Streamer to Fix Spaces and Newlines ---
# Llama's SentencePiece tokenizer often omits spaces, and the generic TextStreamer
# can miss explicit newline tokens (\n, token ID 198) when streaming.
class PoetryStreamer(TextStreamer):
    """
    A custom streamer that ensures proper spacing and forces an immediate
    line break whenever the model generates the newline token (ID 198 for Llama).
    """
    def __init__(self, tokenizer, **kwargs):
        super().__init__(tokenizer, **kwargs)
        # Llama's single newline token is typically ID 198
        self.newline_token_id = tokenizer.convert_tokens_to_ids('\n') 
        # Token cache is used to build up partial words/bytes
        self.token_cache = [] 
        self.print_len = 0 # Tracks how much of the output has been printed

    def on_token_stream(self, token: int) -> None:
        """Called for every token generated by the model."""
        
        # Check for the explicit newline token
        if token == self.newline_token_id:
            # Force a line break, flush immediately, and clear the cache
            print('\n', end='', flush=True)
            self.token_cache = [] 
            self.print_len = 0
            return # Skip further processing for this token
        
        # Add the token to the cache
        self.token_cache.append(token)
        
        # Decode the current cache with the crucial `clean_up_tokenization_spaces=True`
        output = self.tokenizer.decode(
            self.token_cache, 
            skip_special_tokens=self.skip_special_tokens,
            # This is the key to fixing the word-stitching (e.g., "mynameis")
            clean_up_tokenization_spaces=True 
        )

        # Print only the new part of the output
        new_output = output[self.print_len :]
        
        # If the decoded output doesn't end in a space, it means we have a partial word
        # (e.g., 'm' or 'my') and we shouldn't print and clear the cache yet.
        # Otherwise, if it ends in a space, we can confidently print the whole buffer.
        if output and (output.endswith(" ") or len(output) > 100):
            self.on_text(new_output)
            self.token_cache = []
            self.print_len = 0
        else:
            self.print_len += len(new_output)


# --- Configuration ---
MODEL_NAME = "meta-llama/Meta-Llama-3.1-8B"
ADAPTER_PATH = r"D:\models\llama-3-8b-q4-finetuned-poetry-mercury-11-test-001-r64-alpha128-3e4-lngth1024-50epochs\final_model" 

# Quantization config (4-bit NF4 with bfloat16 compute)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# --- Model Loading ---
print("--- Starting Model and Tokenizer Loading (This may take a moment) ---")
try:
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    # NOTE: Llama 3 uses a different token for padding/EOS, but setting it this way is common practice
    tokenizer.pad_token = tokenizer.eos_token 
    
    # Load quantized base model
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",
        dtype=torch.bfloat16 
    )
    base_model.eval()

    # Load your adapter onto the base model
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    model.eval() 

    # Device check
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"--- Models loaded successfully on device: {device} ---")

except Exception as e:
    print(f"\n--- ERROR during model loading or configuration! ---")
    print(f"Details: {e}")
    sys.exit(1)

# --- Interactive Generation Loop ---

# **IMPORTANT CHANGE**: Initialize the streamer with the custom class
streamer = PoetryStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) 

# Generation arguments shared between both models
GEN_KWARGS = dict(
    min_length=50,
    max_length=500,
    num_return_sequences=1,
    do_sample=True,
    top_k=250, 
    top_p=0.95,
    temperature=1.25,
    repetition_penalty=1.05,
    pad_token_id=tokenizer.eos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    # The streamer handles the output printing, so we don't need to return the full sequences
    return_dict_in_generate=True,
    output_scores=True
)

print("\n\n--- Interactive Poetry Generation Mode Activated! POESIA DI MICHELE BOTTARI---")
print("Type a starting line for your poem (e.g., 'Coming upon the lake a night, we').")
print("Type 'quit' or 'exit' to stop the program.")
print("--------------------------------------------------")

# Start the main interactive loop
while True:
    try:
        # Get user input
        prompt = input("\nYour Prompt (or 'quit'/'exit'): ")
        
        # Check for exit commands
        if prompt.lower() in ["quit", "exit"]:
            print("\nExiting interactive mode. Goodbye! ðŸ‘‹")
            break 
        
        # Check if the prompt is empty
        if not prompt.strip():
            print("Please enter a non-empty prompt.")
            continue
        
        # --- Generation Setup ---
        # add_special_tokens=False is VITAL for a clean prompt feed with Llama-3/PEFT
        input_ids = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=False).to(device)

        with torch.no_grad():
            
            # 1. FINETUNED MODEL GENERATION (STREAMED)
            print(f"\n{'='*70}")
            print(f"--- FINETUNED MODEL (POESIA DI MICHELE BOTTARI) OUTPUT ---")
            print(f"Prompt: '{prompt}'")
            print(f"{'-'*70}")
            # Pass the streamer to quitstream the output to stdout
            model.generate(
                input_ids,
                streamer=streamer, 
                **GEN_KWARGS
            )
            print(f"\n{'='*70}\n")
            
            # 2. BASE MODEL GENERATION (STREAMED)
            print(f"\n{'='*70}")
            print(f"--- BASE MODEL OUTPUT (meta-llama/Meta-Llama-3.1-8B) ---")
            print(f"Prompt: '{prompt}'")
            print(f"{'-'*70}")
            # Use the base_model instance and pass the same streamer
            base_model.generate(
                input_ids,
                streamer=streamer, 
                **GEN_KWARGS
            )
            print(f"\n{'='*70}\n")


        print("\n\n--- Generation Cycle Finished (Finetuned & Base) ---")
        # Loop restarts, asking for a new prompt

    except KeyboardInterrupt:
        print("\n\nExiting interactive mode via Keyboard Interrupt. Goodbye! ðŸ‘‹")
        break
    except Exception as e:
        print(f"\nAn unexpected error occurred during generation: {e}")
        print("Please try a different prompt or restart the program.")
        continue