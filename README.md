# üåí **scritti**  
### *Applied LLM Experimentation, Evaluation, and Fine-Tuning*  

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)  
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)  
[![LLM R&D](https://img.shields.io/badge/Focus-LLM%20Evaluation%20%26%20Fine--Tuning-purple.svg)]()  
[![AI Governance](https://img.shields.io/badge/Discipline-AI%20Governance-orange.svg)]()

---

## üß≠ **Purpose & Professional Context**

This repository showcases my hands-on work in **LLM evaluation, dataset tooling, prompt orchestration, and small-scale fine-tuning** ‚Äî the same domains I manage professionally as a **Technical Program Manager specializing in AI Governance & Applied AI Operations**.

While my day-job focuses on *risk management, evaluation pipelines, and GenAI program execution*, this repo captures the **personal R&D work** I do to deepen the engineering/ML side of that discipline.

**In short:**  
> This is where I experiment with the models, metrics, and behaviors that I govern, evaluate, and operationalize in production environments.

---

## üß© **What This Repository Demonstrates**

This repo is intentionally diverse ‚Äî it contains a portfolio of practical, minimally-packaged scripts that demonstrate:

### üõ†Ô∏è Technical Engineering Capabilities
- Python tooling (Pandas, tokenization, JSON parsing, I/O utilities)  
- Prompt orchestration logic  
- Custom evaluation metrics (e.g., early work on iambic pentameter scoring)  
- GPT-2 fine-tuning experiments with partial layer unfreezing  
- Quantization and optimization on limited hardware (16GB VRAM)  
- Dataset formatting utilities (for supervised fine-tuning or RAG prep)

### üß™ AI Governance & Model Lifecycle Relevance
- Understanding of *why* models behave incoherently  
- Ability to perform controlled prompt-based evaluations  
- Failure-mode analysis (semantic drift, instability, overfitting)  
- Logging patterns that mirror internal evaluation pipelines  
- Experiment reproducibility practices  

### üìà Program Management & Systems Thinking
- Translating ambiguous goals into measurable experiments  
- Structuring AI workflows with clear inputs/outputs  
- Aligning fine-tuning work with model safety/evaluation concerns  

---

## ü§ñ **New: Mercury ‚Äî Local Speech-Enabled LLM Agent**

A new addition to the repository: **Mercury**, a fully local, speech-enabled LLM agent built on my merged GPT-2 fine-tuned poetry model.

This agent demonstrates how to combine **local model inference**, **offline STT**, **memory-bounded conversational context**, and **agent-style prompting** into a cohesive interactive tool. It also reflects real model-ops considerations like device-aware loading, error handling, and multimodal input.

---

## üìÅ Directory: `gpt2-files/local-agent-001/`

```text
local-agent-001/
‚îú‚îÄ‚îÄ main.py                          # Full agent (text + optional voice input)
‚îú‚îÄ‚îÄ main-voice-input-only.py         # Voice-only simplified entry point
‚îú‚îÄ‚îÄ requirements.txt                 # Dependencies (transformers, vosk, sounddevice, torch, etc.)
‚îú‚îÄ‚îÄ test_cuda.py                     # Quick GPU/torch diagnostic

‚îú‚îÄ‚îÄ agent-mercury/                   # Agent logic, utilities, prompt handling
‚îÇ   ‚îî‚îÄ‚îÄ ...                          # (Modularized helper scripts)

‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ full_merged_gpt2-finetuned-poetry-mercury-04--copy-attempt/
‚îÇ       ‚îú‚îÄ‚îÄ config.json
‚îÇ       ‚îú‚îÄ‚îÄ generation_config.json
‚îÇ       ‚îú‚îÄ‚îÄ merges.txt
‚îÇ       ‚îú‚îÄ‚îÄ model.safetensors
‚îÇ       ‚îî‚îÄ‚îÄ vocab.json               # Fine-tuned + merged GPT-2 weights

‚îî‚îÄ‚îÄ vosk-model-en-us-0.22/           # Offline speech-recognition model for voice mode
```

---

## üß† Mercury Agent Overview

Mercury is powered by the **fine-tuned GPT-2 poetry model** from this repo, combined with:

- **Offline speech recognition** using Vosk  
- **Dynamic conversation memory** via bounded `deque`  
- **Personality-driven system prompt** (‚Äúancient poet and insightful assistant‚Äù)  
- **Ghost-word cleanup** for noisy speech transcripts  
- **Device-aware model loading** (GPU if available, CPU fallback)  
- **Robust generation loop** with sampling + repetition penalties  
- **Command interface** for managing agent state

This brings together multiple areas of experimentation from the repo ‚Äî generation, fine-tuning, evaluation logic, and applied inference.

---

## ‚ñ∂Ô∏è Running the Agent

### **Text + Voice Mode (full agent)** - (Note: this one is under construction)
```bash
python main.py
```

### **Voice-Only Mode** - (Note: this one is the most operational currently)
```bash
python main-voice-input-only.py
```

---

## üé§ Voice Commands

When running the full agent (`main.py`):

- `/voice` ‚Äî Toggle voice input (requires Vosk model loaded)  
- `/clear` ‚Äî Reset conversation memory  
- `/exit` ‚Äî Quit cleanly  
- `/quit` or `/bye` ‚Äî Same as above  

---

## üß© Example Functionality

- Listens for offline speech, cleans filler words, and converts to text  
- Builds a prompt using a sliding window of conversation memory  
- Generates responses using the fine-tuned GPT-2 model  
- Removes prompt leakage and ensures clean assistant output  
- Falls back to typed input if the microphone is silent  
- Warns and adjusts automatically if CUDA is unavailable

---

## üìÅ **Repository Structure**

```text
üìÅ scritti/
‚îú‚îÄ‚îÄ üìÑ README.md
‚îú‚îÄ‚îÄ üìÑ First_Edition_GenPs-001_10_14_25.txt

‚îú‚îÄ‚îÄ üìÅ gpt2-files/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ generation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ gpt2-generate-iambic-pentameter-003.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ gpt2-generation-haiku_form-002.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ interactive-poetry-chat-in-terminal-002--gpt2-with-comparison.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ the-gpt2-fine-tuning-script-thats-the-best-tweaked-002-unfreeze-top-layers--chatbot-compare.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ tuning/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ the-gpt2-fine-tuning-script-thats-the-best-tweaked-003-unfreeze-top-layers---keep-source-line-breaks.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ local-agent-001/
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ main.py
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ main-voice-input-only.py
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ requirements.txt
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ test_cuda.py
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ agent-mercury/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ... (agent logic, prompt handling, utilities)
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ models/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ full_merged_gpt2-finetuned-poetry-mercury-04--copy-attempt/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ config.json
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ generation_config.json
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ merges.txt
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ model.safetensors
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ vocab.json
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ üìÅ vosk-model-en-us-0.22/
‚îÇ           ‚îî‚îÄ‚îÄ ... (offline STT model files)

‚îî‚îÄ‚îÄ üìÅ llama-files/
    ‚îú‚îÄ‚îÄ üìÅ generation/
    ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ interactive-poetry-chat-in-terminal-for-llama3-002-with-comparison.py
    ‚îî‚îÄ‚îÄ üìÅ tuning/
        ‚îî‚îÄ‚îÄ üìÑ fine-tuning-script-for-llama-3-q4-001.py
```

### **`First_Edition_GenPs-001_10_14_25.txt`** ‚Äî Results examples
Examples of results from using these scripts for fine-tuning experiments.

### **`/gpt2-files/`** ‚Äî GPT-2 experiments
- **`generation/`**: Scripts for generating poetry in various forms (iambic pentameter, haiku) and interactive chat interfaces with model comparison
- **`tuning/`**: Fine-tuning scripts with partial layer unfreezing and line break preservation

### **`/llama-files/`** ‚Äî Llama 3 experiments  
- **`generation/`**: Interactive poetry chat terminal for Llama 3 with comparison functionality
- **`tuning/`**: Fine-tuning scripts optimized for Llama 3 with 4-bit quantization

---

## üß† **Why This Repo Exists**

> I created **scritti** to make transparent that ‚Äî despite building my career as a Technical Program Manager ‚Äî I am also deeply hands-on with the *engineering and experimental* side of AI.

Where my r√©sum√© emphasizes:
- **risk mitigation**  
- **requirements definition**  
- **AI safety operations**  
- **cross-functional program delivery**

‚Ä¶this repo highlights the *technical underpinnings* I continue to develop:
- Model behavior analysis  
- Evaluation design  
- Scripting tooling  
- Experimentation discipline  

Together, they illustrate how I bridge **AI governance ‚Üî applied engineering**.

---

## üî• **Featured Work: GPT-2 Fine-Tuning (Before vs After)**

Below is a real output comparison between **GPT-2** and my **fine-tuned model** using the same input prompt and generation settings. The fine-tuned model was trained to capture a distinct poetic style, producing surreal and borderline nonsensical imagery rather than conventional narrative, demonstrating the effect of stylistic fine-tuning.

<details>
<summary><strong>üß† BASE (GPT2) MODEL OUTPUT üß†</strong> (click to expand)</summary>
<br/>
watching with glacial gravity isn't life. She knew he wasn't gay. What they didn't know, until her the hospital stopped billing him for $75 a week‚Äîtwo or three quarters of what he paid in back fees‚Äîwas that she was pushing him toward that home over there where the men let more money run through the plumbing and had greater bandwidth for the conversation, by contrast, if someone had only run an electric line down in time and slid money back into himself through life.

"She was someone of stature and quality, still," Drew Alexander continues. "[He] wanted someone nice. Nice how people say, if it was to just tell the next one about, I don't think the price would ever<br>
 work because you never tell anybody that you work in white tape after 'hand money,' I'm happy to work in handshake money for, ya know, sorta like their life... He could be spooked by all the pushy parties. Or who would believe this is a coincidence, who would allow himself to be cuckolded, his wife<br>
 cheated on by the roommate‚Ä¶ they always blamed [The Purge writer] Jackson, nothing could have survived outside the U.K.s conditions like that."[10] But according to Andrew Maguire (TAS and prolific Purge eponym), Alexander's willingness to play with facts wasn't seen at the 2007 Indianapolis premiere for his 2009 instalment Left Me Stand Alone when authorities declared white trash vans roaming the heart of town were highly controversial responses to drug rapes. "Some evil little drug cunts like
 Margo Irving should be responsible for such barbaric behavior because she let her child be hit by a<br>
 semi-maimed alcoholic male ‚Ä¶ they fought every step of the way like this when under the influence that proves how cheap were they and still are." Every one. [F/L/G is valid opinions]. As women the environment rarely deterr the vigilante elements.[Orness suggests the oldman played you falsely... as always, heart][Image so wonderfully bare one where gore is spilled.[Male depiction?]Now wait a minute- Apparently just leaving this low bitch tag intact[Still "nice how you know it!" show] http://bit.ly/8VA91U . Ever spend all week this Newer standard bill containing $1 MillionFPS HOF win awards the<br>
 boy whom to love was connected: articleIt also said a fuck even more popular.Time: How Lit. 19 r i D ************* *****U t/H dade.ruskus,last sings on so donner 35S ALLR ins good word well g l iv do<br>
 r all on imafl,we the skingo.alevich Suck better price milano butget like assemede sonodaisnnot me second free phone thisc:1k800 an infinity 13 j u let frather' 11D freesia play the negard Stakedhell<br>
 But three popindteddu-four fine answers ifeven commacedte1ibands,"$37636 O.O baic ... D any chance you can make an honorable answer?5 out963 plus number".Keep records it is important sish sayitsprayshop checks false abuse."They became ‚Ä¶ James Row and so deseewedhip he marm a call had could too vei-<br>
 alsostoterseaktion nehow much that night on bodymapaysha servicethey chekepsed hot stay feel os back these G iv get mad that butse inside my yors. Seen my pro CASS WebletIt was down still which rhou d eherec with nuk

</details>

<details>
<summary><strong>üé≠ FINETUNED (POESIA DI MICHELE BOTTARI) MODEL OUTPUT üé≠</strong> (click to expand)</summary>
</br>
watching with glacial intensity <br>
pretending not to notice, like you can do it <br>
without thoughts, without trying, <br>
like two trapped souls in searing and twisting <br>
lens shadows. <br>
<br>
<br>
I'm sorry, I don't know what else to say. <br>
        I never knew that you could be so loving, <br>
even when things are hard, when the sun <br>
and the empty streets hang motionless <br>
in a pall of gray, and <br>
the mourners in white sit on gray <br>
trees writhing in the cool air <br>
but I now understand <br>
what it means to feel the depth of <br>
a loved one's sorrow, to understand <br>
how one's love can survive <br>
the ravages of time, the rigors of <br>
deserts and fires and tornados, <br>
and how we can stop believing <br>
when everything else crumbles, and <br>

</details>

---

## üß∞ **Key Technologies Used**

- Python (tooling, parsing, orchestration)
- PyTorch
- Transformers (Hugging Face)
- TensorBoard
- LoRA adapters
- Quantization / 8-bit loading
- Dataset cleaning and text normalization
- Prompt engineering & stylistic constraints

---

## üõ†Ô∏è **Tooltips / Code Glossary**

**üîß LoRA** <sub>Low-Rank Adaptation: freezes base model weights and learns small rank-decomposition matrices for efficient fine-tuning.</sub>

**üéöÔ∏è Top-k / Top-p Sampling** <sub>Decoding strategies that control randomness by limiting token selection to the highest-probability candidates.</sub>

**üß± Unfreezing Layers** <sub>Gradually enabling gradient updates in deeper transformer blocks to increase control without overfitting.</sub>

**üßÆ Iambic Pentameter Metric** <sub>A custom evaluator that counts unstressed/stressed syllable alternation ‚Äî an experiment in stylistic constraint scoring.</sub>

---

## üó∫Ô∏è **Project Roadmap**

- Add automated metric visualizations
- Add a full reproducible fine-tuning pipeline
- Add more evaluation scripts (e.g., perplexity tracking)
- Add a prompt-stability benchmarking suite
- Add LoRA-based adapters for Llama 3.1 8B

---

## üìö **How This Aligns With My Resume**

| Resume Theme                 | What This Repo Shows                                            |
| ---------------------------- | --------------------------------------------------------------- |
| AI Governance                | Behavior analysis, evaluation logic, failure-mode understanding |
| Technical Program Management | Structured experiments, reproducibility, workflow design        |
| Python Tooling               | Scripts for formatting, parsing, evaluation                     |
| Model Evaluation             | Direct comparison harnesses + qualitative assessments           |
| LLM Training Knowledge       | Fine-tuning code, quantization, training loops                  |
| Cross-Functional Bridging    | Clear documentation, transparency, reproducible outputs         |

---

## üë§ **About Me**

**Michael Bottari**  
Technical Program Manager ‚Äî AI Governance & Applied AI Operations  
üìß [mfbottari@gmail.com](mailto:mfbottari@gmail.com)  
üîó [LinkedIn](https://linkedin.com/in/michael-bottari)  
üíª [GitHub](https://github.com/bottari)

---

## üìù **License**

MIT License ‚Äî feel free to fork, study, and experiment.